/*
 * Copyright (c) 2012 Diamond Light Source Ltd.
 *
 * All rights reserved. This program and the accompanying materials
 * are made available under the terms of the Eclipse Public License v1.0
 * which accompanies this distribution, and is available at
 * http://www.eclipse.org/legal/epl-v10.html
 */
package org.dawnsci.passerelle.cluster.actor;

import java.io.File;
import java.io.FileReader;
import java.io.Reader;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import org.dawb.passerelle.common.DatasetConstants;
import org.dawb.passerelle.common.message.IVariable;
import org.dawb.passerelle.common.message.IVariable.VARIABLE_TYPE;
import org.dawb.passerelle.common.message.IVariableProvider;
import org.dawb.passerelle.common.message.Variable;
import org.dawb.passerelle.common.parameter.ParameterUtils;
import org.dawnsci.passerelle.cluster.service.ScalarNames;
import org.eclipse.dawnsci.analysis.api.message.DataMessageComponent;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import ptolemy.data.expr.StringParameter;
import ptolemy.kernel.CompositeEntity;
import ptolemy.kernel.util.IllegalActionException;
import ptolemy.kernel.util.NameDuplicationException;

import com.isencia.passerelle.actor.ProcessingException;
import com.isencia.passerelle.actor.v5.Actor;
import com.isencia.passerelle.actor.v5.ActorContext;
import com.isencia.passerelle.actor.v5.ProcessRequest;
import com.isencia.passerelle.actor.v5.ProcessResponse;
import com.isencia.passerelle.core.ErrorCode;
import com.isencia.passerelle.core.Port;
import com.isencia.passerelle.core.PortFactory;
import com.isencia.passerelle.message.ManagedMessage;
import com.isencia.passerelle.message.MessageException;

/**
 * This actor reads a job specification file for an analysis workflow job.
 * Main required data is about specifying the data slice, but extra arguments can also be present.
 * All are set as scalars in the DataMessageComponent generated by this actor.
 * 
 * @author erwindl
 *
 */
public class JobSliceSource extends Actor implements IVariableProvider {
  private static final long serialVersionUID = 3078492950853091843L;
  private final static Logger LOGGER = LoggerFactory.getLogger(JobSliceSink.class);

  public Port output;

  /**
   * defines the file where the actor must read the job's input slice definition
   */
  public StringParameter sliceFileParameter;

  public JobSliceSource(CompositeEntity container, String name) throws IllegalActionException, NameDuplicationException {
    super(container, name);
    output = PortFactory.getInstance().createOutputPort(this);
    sliceFileParameter = new StringParameter(this, "Slice file");
  }

  @Override
  public Logger getLogger() {
    return LOGGER;
  }

  @Override
  public List<IVariable> getInputVariables() {
    return Collections.emptyList();
  }

  @Override
  public List<IVariable> getOutputVariables() {
    final List<IVariable> ret = new ArrayList<>(5);
    ret.add(new Variable(ScalarNames.WORKINGDIR, VARIABLE_TYPE.SCALAR, "working directory", String.class));
    ret.add(new Variable(ScalarNames.DATASET, VARIABLE_TYPE.SCALAR, "dataset name", String.class));
    ret.add(new Variable(ScalarNames.SLICE, VARIABLE_TYPE.SCALAR, "slice in NCD compatible format", String.class));
    ret.add(new Variable(ScalarNames.FILENAME, VARIABLE_TYPE.SCALAR, "data file name", String.class));
    ret.add(new Variable(ScalarNames.FILEPATH, VARIABLE_TYPE.SCALAR, "path of the data file", String.class));

    return ret;
  }

  @Override
  protected void process(ActorContext ctxt, ProcessRequest request, ProcessResponse response) throws ProcessingException {
    try {
      Properties jobSpec;
      File jobSpecFile = null;
      try {
        String jobSpecFileStr = substituteSystemProps(sliceFileParameter);
        jobSpecFile = new File(jobSpecFileStr);
        jobSpec = getJobSpec(jobSpecFile);
      } catch (Exception e) {
        throw new ProcessingException(ErrorCode.ACTOR_EXECUTION_ERROR, "Error getting slice file from parameter", this, e);
      }
      if (jobSpec != null) {
        ManagedMessage resultMsg = createMessage();
        final DataMessageComponent comp = new DataMessageComponent();
        comp.putScalar(ScalarNames.WORKINGDIR, jobSpecFile.getParent());
        for (String propName : jobSpec.stringPropertyNames()) {
          comp.putScalar(propName, jobSpec.getProperty(propName));
        }

        try {
          resultMsg.setBodyContent(comp, DatasetConstants.CONTENT_TYPE_DATA);
          response.addOutputMessage(output, resultMsg);
        } catch (MessageException e) {
          response.setException(new ProcessingException(ErrorCode.MSG_CONSTRUCTION_ERROR, "Failed to create result msg for " + jobSpecFile, this, e));
        }
      } else {
        getLogger().warn("No job specification found in " + jobSpecFile);
      }
    } finally {
      requestFinish();
    }
  }

  private String substituteSystemProps(StringParameter param) throws Exception {
    Properties properties = System.getProperties();
    Map<String, String> map = new HashMap<String, String>();
    for (final String name : properties.stringPropertyNames())
      map.put(name, properties.getProperty(name));

    return ParameterUtils.getSubstituedValue(param,null,map);
  }

  private Properties getJobSpec(File jobSpecFile) {
    Properties props = new Properties();
    if (jobSpecFile.exists()) {
      Reader fileRdr = null;
      try {
        fileRdr = new FileReader(jobSpecFile);
        props.load(fileRdr);
        return props;
      } catch (Exception e) {
        getLogger().error("Error reading job specification from " + jobSpecFile, e);
      } finally {
        if (fileRdr != null) {
          try {
            fileRdr.close();
          } catch (Exception e) {
          }
        }
      }
    }
    return null;
  }
}
